{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 14/14 [00:11<00:00,  1.22it/s, Batch Train Loss=0.0357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0520\n",
      "Epoch [1/2] Average Train Loss: 0.0573, Average Validation Loss: 0.0520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 14/14 [00:11<00:00,  1.25it/s, Batch Train Loss=0.0308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0411\n",
      "Epoch [2/2] Average Train Loss: 0.0295, Average Validation Loss: 0.0411\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from custom_dataset import CustomImageDataset\n",
    "from training_functions import train_denoising_model, validate_model\n",
    "from torchvision.transforms import Resize\n",
    "from autoencoders import Autoencoder1\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Set environment variables\n",
    "TRAIN_FUNCTION = train_denoising_model\n",
    "MODEL = Autoencoder1\n",
    "EPOCHS = 2\n",
    "DEVICE = \"cpu\"\n",
    "NOISE_FACTOR = 0.1\n",
    "DATASET_PATH = \"/Users/leo/Programming/autoencoder/data/TextImages/train_cleaned\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    Resize((256, 256)),  # Resize all images to 224x224\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "full_dataset = CustomImageDataset(DATASET_PATH, transform=transform)\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_size = int(0.75 * len(full_dataset))\n",
    "valid_size = len(full_dataset) - train_size\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Initialize model, criterion, and optimizer\n",
    "model = MODEL().to(DEVICE)\n",
    "criterion = nn.MSELoss().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and Validation\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    # Training\n",
    "    train_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), desc=\"Processing\", total=len(train_loader), leave=True)  # tqdm for batches\n",
    "    for batch_idx, (data, _) in pbar:\n",
    "        loss = TRAIN_FUNCTION(model, data, NOISE_FACTOR, optimizer, criterion, DEVICE)\n",
    "        train_loss += loss\n",
    "        pbar.set_postfix({'Batch Train Loss': f\"{loss:.4f}\"})\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation\n",
    "    valid_loss = validate_model(model, valid_loader, device=DEVICE)\n",
    "    valid_losses.append(valid_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Average Train Loss: {avg_train_loss:.4f}, Average Validation Loss: {valid_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
